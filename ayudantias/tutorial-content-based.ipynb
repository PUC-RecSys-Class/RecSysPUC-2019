{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecsysContentBased_2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernan4444/diplomado-sistemas-recomendadores/blob/master/Diplomado_Alumno_2019_Sistemas_Recomendadores_3_Content_Based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC-ceGb8LRLT",
        "colab_type": "text"
      },
      "source": [
        "# Práctica de Sistemas Recomendadores 3: Content based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mACJbcW8T35p",
        "colab_type": "text"
      },
      "source": [
        "En este práctico, volveremos a utilizar la biblioteca de Python [sklearn](https://scikit-learn.org/stable/), para aprender sobre 2 algoritmos para recomendación basado en contenidos y de unas herramientas para preprocesar los textos. En particula, este practico verá:\n",
        "\n",
        "* TF-IDF\n",
        "* Word Embeddings\n",
        "* Uso de Stop Words\n",
        "\n",
        "**Ayudantes**: Manuel Cartagena, Andrés Carvallo y Patricio Cerda\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKAqmo5IdQFI",
        "colab_type": "text"
      },
      "source": [
        "# Actividad 1\n",
        "\n",
        "Antes de empezar con la actividad, responder la siguiente pregunta con lo visto en clases\n",
        "\n",
        "**Pregunta:** Explique con sus palabras a qué se refiere con recomendación basada en contenido. En particular responda\n",
        "\n",
        "- ¿Qué datos se utilizan para recomendación basada en contenidos?\n",
        "- Mencione un ejemplo donde sea factible utilizar este tipo de recomendación y justifique.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvDxjWvUdcv8",
        "colab_type": "text"
      },
      "source": [
        "**Respuesta:** COMPLETAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFpEoacrMwQx",
        "colab_type": "text"
      },
      "source": [
        "# Descargando la información\n",
        "\n",
        "Vaya ejecutando cada celda presionando el botón de **Play** o presionando Ctrl+Enter (Linux y Windows) o Command+Enter (Macosx) para descargar las bases de datos.\n",
        "\n",
        "*   Recursos:\n",
        "  * `dictionary.p`\n",
        "  * `dictionary-stemm.p`\n",
        "  * `tfidf_model.p`\n",
        "  * `tfidf_model-stemm.p`\n",
        "*   Dataset:\n",
        "  *  `corpus1.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUlFGZprHneQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descarga de recursos\n",
        "!curl -L -o 'resources.tar.gz' 'https://github.com/PUC-RecSys-Class/Syllabus/blob/master/Practico%204/files/resources.tar.gz?raw=true'\n",
        "\n",
        "# Descompresión del archivo\n",
        "!tar -xvf resources.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN0P2xxrH0z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descarga del dataset\n",
        "!curl -L -o 'dataset.tar.gz' 'https://github.com/PUC-RecSys-Class/Syllabus/blob/master/Practico%204/files/dataset.tar.gz?raw=true'\n",
        "\n",
        "# Descompresión del archivo\n",
        "!tar -xvf dataset.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJon9T5ZMwRG",
        "colab_type": "text"
      },
      "source": [
        "# Revisar archivos descargados\n",
        "\n",
        "Revisemos el _dataset_ descargado:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT11_REYOyFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus_df = pd.read_csv('./corpus1.csv', sep='\\t',\n",
        "                        header=None, encoding='latin',\n",
        "                        names=['id', 'title', 'abstract'])\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pSGsVQkyQoVi"
      },
      "source": [
        "Podemos ver que este _dataet: contiene 3 columnas:\n",
        "* **_id_**: identificador de cada texto\n",
        "* **_title_**: título del documento, en este caso, de un _paper_\n",
        "* **_abstract_**: primer párafo del _paper_ que es una representación abreviada, objetiva y precisa del contenido de un documento o recurso, sin interpretación crítica y sin mención expresa del autor del resumen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HU7NoDUhnYl",
        "colab_type": "text"
      },
      "source": [
        "## Preparar entorno\n",
        "Primero es necesario instalar algunas librerías previas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtscg3KuMwRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk\n",
        "!pip install sklearn\n",
        "!pip install gensim\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrZhH8Kqtx7_",
        "colab_type": "text"
      },
      "source": [
        "Luego necesitamos importar las librerías a utilizar en este práctico. No se asusten por todas las librerías, iremos explicando lo más importante a medida que se avanza en el práctico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ii2pB-LO0Xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "import gensim\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "from collections import Counter\n",
        "from os.path import isfile\n",
        "from textwrap import wrap\n",
        "\n",
        "from gensim import corpora, models, similarities\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "import json\n",
        "import warnings\n",
        "import gensim \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.options.display.max_columns = None\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUYnjZ1yOY-A",
        "colab_type": "text"
      },
      "source": [
        "## Preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo0B-gBrR9Sf",
        "colab_type": "text"
      },
      "source": [
        "Volvemos a cargar el _dataset_ a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bONZGsYBR_rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df = pd.read_csv('./corpus1.csv', sep='\\t',\n",
        "                        header=None, encoding='latin',\n",
        "                        names=['id', 'title', 'abstract'])\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me-LXrP2Ocjc",
        "colab_type": "text"
      },
      "source": [
        "Luego descargamos las librerías de NLTK necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ru8N7mZ9exU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUTeZc5jSoIG",
        "colab_type": "text"
      },
      "source": [
        "En este momento estamos bajando un _tokenizador_ específico llamado [Punkt Sentence Tokenizer](https://kite.com/python/docs/nltk.tokenize.punkt). Este será usado a continuación para realizar una cierta tarea con los textos (no vamos a decir cual es porque una actividad es que comenten que hace dado unos ejemplos que mostramos c: ). \n",
        "\n",
        "Lo siguiente es implementar una función que transforme texto no estructurado a una lista de *tokens* procesados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25JLYOGGSOck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens(text):\n",
        "    # Pasar todo a minuscula\n",
        "    lowers = text.lower()\n",
        "    \n",
        "    # Quitar puntuación\n",
        "    no_punctuation = lowers.translate({ord(c): None for c in string.punctuation})\n",
        "    \n",
        "    # Tokenizar \n",
        "    tokens = nltk.word_tokenize(no_punctuation)\n",
        "    \n",
        "    # Retornar resultado\n",
        "    return tokens\n",
        "\n",
        "\n",
        "print(get_tokens(\"I'm a super student for recommender systems!\"))\n",
        "print(get_tokens(\"First sentence. Seconde sentence.\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckrxbKlTUVJ8",
        "colab_type": "text"
      },
      "source": [
        "En el código anterior, para ejecutar `nltk.word_tokenize()` era necesario tener descargado _punkt_. \n",
        "\n",
        "## Actividad 2\n",
        "\n",
        "En función a las frases ingresadas y al resultado impreso, ¿Qué significa _Tokenizar_?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpFOeSpET3BL",
        "colab_type": "text"
      },
      "source": [
        "**Respuesta:** COMPLETAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gifqhG3_ZO0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A cada abstract le aplicamos la función de get_tokens\n",
        "corpus_df['tokenized_abstract'] = corpus_df.abstract.map(get_tokens)\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkraQ1AwWT86",
        "colab_type": "text"
      },
      "source": [
        "Ahora se tiene que generar un diccionario con todas las palabras del *corpus*. Se recomienda revisar la documentación de gensim y leer cómo usar los diccionarios: [corpora.dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29j2p32oTTzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_file = './resources/dictionary.p'\n",
        "\n",
        "if isfile(dict_file): # Verificar si existe el archivo\n",
        "    dictionary = corpora.dictionary.Dictionary().load(dict_file)\n",
        "    \n",
        "else: # En otro caso, crear el archivo y guardarlo\n",
        "    dictionary = corpora.dictionary.Dictionary(documents=corpus_df.tokenised_abstract.tolist())\n",
        "    dictionary.save(dict_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEDzoAtbZdO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Texto original\n",
        "print(\"Texto 1\")\n",
        "wrap(str(corpus_df.loc[0][\"tokenized_abstract\"]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDJuYbHuY2rX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Texto pasado por el diccionario\n",
        "print(\"Texto 1\")\n",
        "wrap(str(dictionary.doc2bow(corpus_df.loc[0][\"tokenized_abstract\"])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Tbu3OUZ_qo",
        "colab_type": "text"
      },
      "source": [
        "Cuando se hizo `dictionary.doc2bow` se transformó una lista de palabas a un contador de ellas. En donde cada tupla representa `(ID, cantidad de veces)` de modo que se reduce la cantidad de palabras del texto a información numerica. \n",
        "\n",
        "Por ejemplo, la tupla `(30, 5)` indica que la palabra con ID 30 está 5 veces en el texto. Revisando el texto podemos ver que la palabra **\"a\"** es la que está repetida 5 veces. Esto implica que **\"a\"** está asignada al ID 30.\n",
        "\n",
        "Ahora aplicaremos esta función a cada texto del _dataset_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvEkoc9_ZsLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df['bow'] = corpus_df.tokenized_abstract.map(dictionary.doc2bow)\n",
        "\n",
        "corpus = corpus_df['bow'].tolist()\n",
        "\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f23GriULTHgV",
        "colab_type": "text"
      },
      "source": [
        "# Tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQqcAAVrfwZw",
        "colab_type": "text"
      },
      "source": [
        "Recordemos que Tf-idf es una medida numérica que expresa cuán relevante es una palabra para un documento en una colección. Ahora, dada la frecuencia de cada palabra en cada texto, se v a utilizar esta ténica para obtener tuplas de la forma `(ID, Tf-idf)` en donde ID será el ID de la palabra igual como estaba antes (por ejemplo **\"a\"** tiene ID 30) y Tf-Idf será el valor dado por este algoritmo a la palabra en cuestión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ju5n3xTKtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_model_file = 'resources/tfidf_model.p'\n",
        "\n",
        "if isfile(tfidf_model_file):\n",
        "    tfidf_model = models.tfidfmodel.TfidfModel().load(tfidf_model_file)\n",
        "\n",
        "else:\n",
        "    tfidf_model = models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary)\n",
        "    tfidf_model.save(tfidf_model_file)\n",
        "\n",
        "corpus_df['tf_idf'] = tfidf_model[corpus_df.bow.tolist()]\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU39mygGW8P9",
        "colab_type": "text"
      },
      "source": [
        "## Generar recomendaciones: \n",
        "En esta sección se implementan las funciones necesarias para poder generar recomendaciones dado lo que un usuario ha consumido. De manera artificial, se \"samplearán\" 3 documentos aleatorios que representarán al usuario objetivo (`sample`). Luego tendrás que generar diferentes recomendaciones y evaluar los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzolZRLmWnOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random users\n",
        "samples = corpus_df.sample(3)\n",
        "samples_ids = []\n",
        "\n",
        "for n, (ix, paper) in enumerate(samples.iterrows()):\n",
        "    samples_ids.append(ix)\n",
        "    idx, title, abstract, bow, tf_idf = paper[[\n",
        "        'id', 'title', 'abstract', 'bow', 'tf_idf']]\n",
        "    print('%d) %s' % (n+1, title))\n",
        "    print('')\n",
        "    print(\"\\n\".join(wrap(abstract)))\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOgxiXx2XIkn",
        "colab_type": "text"
      },
      "source": [
        "Lo anterior son 3 textos tomados al azar. Asumiremso que una persona vió estos 3 textos y ahora vamos a recomendarle 5 nuevos por cada documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et3iltfFWnW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recommendation functions\n",
        "\n",
        "N = len(dictionary)\n",
        "\n",
        "\n",
        "def to_sparse(matrix):\n",
        "    return csr_matrix([\n",
        "        gensim.matutils.sparse2full(row, length=N)\n",
        "        for row in matrix\n",
        "    ])\n",
        "\n",
        "\n",
        "def make_recommendations(model, metric, neighbors):\n",
        "    M = len(corpus)\n",
        "\n",
        "    X = to_sparse(corpus_df[model].tolist())\n",
        "    document_index = NearestNeighbors(\n",
        "        n_neighbors=(neighbors + 1),\n",
        "        algorithm='brute',\n",
        "        metric=metric).fit(X)\n",
        "    return document_index\n",
        "\n",
        "\n",
        "def print_recommendations(indexes, model):\n",
        "    for n, (ix, paper) in enumerate(samples.iterrows()):\n",
        "        dists, neighbors = indexes.kneighbors([gensim.matutils.sparse2full(paper[model], length=N)])\n",
        "        print(paper['title'])\n",
        "        print('')\n",
        "        print('Documentos cercanos: ')\n",
        "        i = 1\n",
        "        for neighbour in neighbors[0]:\n",
        "            if ix != neighbour:\n",
        "                line = str(i) + \". \" + corpus_df.iloc[neighbour]['title']\n",
        "                print(line)\n",
        "                i += 1\n",
        "        print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmgISQfTXNXB",
        "colab_type": "text"
      },
      "source": [
        "A continuación deberá utilizar las funciones implementadas anteriormente para generar nuevas recomendaciones variando los parámetros del modelo. **Agregue nuevas celdas para cada implementación y/o pregunta.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI9OZ52IWndk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recommendation example: TF-IDF\n",
        "doc_idx = make_recommendations('tf_idf', 'euclidean', 5)\n",
        "print_recommendations(doc_idx, 'tf_idf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfInn_xVSmZ6",
        "colab_type": "text"
      },
      "source": [
        "# Stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LgcEzIcxM4W",
        "colab_type": "text"
      },
      "source": [
        "A continuación, intentaremos mejorar los resultados obtenidos con TF-IDF eliminando las *stopwords*. ¿Qué son las *stopwords*? Son palabras vacías, sin significado, que no aportan (de manera significativa) al sentido de una frase, como los artículos, pronombres, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNlhgqHaVZuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTKiDBwpoJ4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    filtered_words = [\n",
        "        word for word in text if word not in stopwords.words('english')\n",
        "    ]\n",
        "    return filtered_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQFhKTk1xM4p",
        "colab_type": "text"
      },
      "source": [
        "Ahora eliminamos los stopwords de los textos y volvemos a hacer todo el proceso pero con textos diferentes. Este proceso dura aproximadamente **5 minutos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqQRTEtiUf1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Puede que se demore un poco esta celda\n",
        "corpus_df['tokenized_abstract_without_stopwords'] = corpus_df.tokenized_abstract.map(remove_stopwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qf1SZ8DRQ6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsAcYiECX0FO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_df['bow_without_stopwords'] = corpus_df.tokenized_abstract_without_stopwords.map(dictionary.doc2bow)\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2CWQqabaUFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = corpus_df['bow_without_stopwords'].tolist()\n",
        "\n",
        "tfidf_model_file_without_stopwords = 'resources/tfidf_model.p'\n",
        "\n",
        "if isfile(tfidf_model_file):\n",
        "    tfidf_model_without_stopwords = models.tfidfmodel.TfidfModel().load(tfidf_model_file)\n",
        "\n",
        "else:\n",
        "    tfidf_model_without_stopwords = models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary)\n",
        "    tfidf_model_without_stopwords.save(tfidf_model_file_without_stopwords)\n",
        "\n",
        "corpus_df['tf_idf_without_stopwords'] = tfidf_model_without_stopwords[corpus_df.bow_without_stopwords.tolist()]\n",
        "corpus_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXj_9I_JitXw",
        "colab_type": "text"
      },
      "source": [
        "**Actividad:** Genere recomendaciones para un nuevo usuario utilizando los nuevos vectores generados sin stop-words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6IAj7wii_OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxqEz_S0ensc",
        "colab_type": "text"
      },
      "source": [
        "# Word Embeddings\n",
        "En esta sección haremos recomendacion de textos médicos de [PubMed](https://www.ncbi.nlm.nih.gov/pubmed/) que han sido revisados por expertos. \n",
        "\n",
        "RESPONDER LAS SIGUIENTES PREGUNTAS: \n",
        "- ¿Que son word embeddings? ¿Cuál es la intuición?\n",
        "- ¿Por qué son útiles para representar documentos?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vnYP9TQSdfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descarga de recursos\n",
        "!wget https://www.dropbox.com/s/gc3x9rp4gu2tmch/documents_w2vec.json.zip\n",
        "!unzip documents_w2vec.json.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmyJj4cbSdnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descarga del dataset\n",
        "!wget https://www.dropbox.com/s/1bxuw3uf3xwyrr7/pubmed_data.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HY0AggcTvH0",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que este _dataet: contiene 4 columnas:\n",
        "* **user_id**: identificador de cada usuario \n",
        "* **pid**: identificador de cada texto con su correlativo de PubMed. \n",
        "* **_title_**: título del documento, en este caso, de un _paper_\n",
        "* **_abstract_**: primer párafo del _paper_ que es una representación abreviada, objetiva y precisa del contenido de un documento o recurso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dKbIzvXTr-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('pubmed_data.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGXXyftCUAOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creamos diccionario de titulos y abstracts que utilizaremos despues\n",
        "dict_title_abstract = {}\n",
        "\n",
        "for pid, title, abstract in zip(df.pid, df.title, df.abstract):\n",
        "  dict_title_abstract[pid] = {'title': title, 'abstract': abstract}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE0JMfF4Sd3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cargamos diccionario de embeddings por cada documento (pre-procesado)\n",
        "w2vec_vectors = json.load(open('documents_w2vec.json'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqjurNgZVBHp",
        "colab_type": "text"
      },
      "source": [
        "creamos un objeto *gensim.keyedvectors* para hacer más eficiente la búsqueda de documentos similares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhKPIiaCU-5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 300\n",
        "\n",
        "doc2vec = gensim.models.keyedvectors.Word2VecKeyedVectors(embedding_size)\n",
        "keys = list(w2vec_vectors.keys())\n",
        "values = [\n",
        "    w2vec_vectors[key]\n",
        "    for key in keys\n",
        "]\n",
        "doc2vec.add(keys, values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74V5fvAfY3cQ",
        "colab_type": "text"
      },
      "source": [
        "## Generar recomendaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL7YfnQaUxge",
        "colab_type": "text"
      },
      "source": [
        "función **find_similar** para encontrar documentos similares a un pid en particular que recibe id del documento y los topn documentos mas similares y retorna topn documentos más similares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYpU31trUG6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_similar(pid, topn):\n",
        "  results = []\n",
        "\n",
        "  for id_, score in doc2vec.similar_by_vector(doc2vec[pid], topn=topn):\n",
        "      results.append([id_, score, dict_title_abstract[int(id_)]['title'], dict_title_abstract[int(id_)]['abstract']])\n",
        "\n",
        "  return pd.DataFrame(results[1:], columns = ['pid', 'score', 'title', 'abstract'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hdVsRKTUHCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "find_similar('22508578', 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqovwUr7VNMx",
        "colab_type": "text"
      },
      "source": [
        "función **recommend** para recomendar a un usuario de acuerdo una muestra de documentos que ha leído."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl5-3vZCUHHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recommend(user_id, topn, sample_user):\n",
        "  user_docs =  df[df.user_id==user_id]['pid'].sample(sample_user)\n",
        "\n",
        "  results = []\n",
        "\n",
        "  for pid in user_docs:\n",
        "\n",
        "    for id_, score in doc2vec.similar_by_vector(doc2vec[str(pid)], topn=topn):\n",
        "\n",
        "      if int(id_) in dict_title_abstract:\n",
        "        results.append([id_, score, dict_title_abstract[int(id_)]['title'], dict_title_abstract[int(id_)]['abstract']])\n",
        "    \n",
        "\n",
        "  results = sorted(results, key = lambda x: int(x[1]))\n",
        "\n",
        "  return pd.DataFrame(results[topn:], columns = ['pid', 'score', 'title', 'abstract']).head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN1yayQPUHKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# documentos leidos por el usuario \n",
        "df[df.user_id==348892].sample(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3OgPussUHOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recommend(user_id= 348892, topn= 10, sample_user = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT9KuOZyVsBS",
        "colab_type": "text"
      },
      "source": [
        "RESPONDER:\n",
        "- ¿Qué problemas puede tener la recomendación basada en contenido?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udiRZZeyaino",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}